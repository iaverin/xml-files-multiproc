# xml-files-multiproc

## Задача
Написать программу на Python, которая делает следующие действия:

1. Создает 50 zip-архивов, в каждом 100 xml файлов со случайными данными следующей структуры:
   ```xml
    <root>
        <var name=’id’ value=’<случайное уникальное строковое значение>’/>
        <var name=’level’ value=’<случайное число от 1 до 100>’/>
        <objects>
            <object name=’<случайное строковое значение>’/>
            <object name=’<случайное строковое значение>’/>
            …
        </objects>
    </root>
    ```
    В тэге objects случайное число (от 1 до 10) вложенных тэгов object.

2. Обрабатывает директорию с полученными zip архивами, разбирает вложенные xml файлы и формирует 2 csv файла:
Первый: id, level - по одной строке на каждый xml файл
Второй: id, object_name - по отдельной строке для каждого тэга object (получится от 1 до 10 строк на каждый xml файл)
Очень желательно сделать так, чтобы задание 2 эффективно использовало ресурсы многоядерного процессора.


## Решение

### Как работает
1. Решение создает директорию ```zip-files``` и складывает туда zip файлы с XML . Если директории не было, то она создается. 
2. Zip архивы перезаписываются  и добавляются в ```zip-files```
3. Результат работы: ```csv_file_1.csv``` и ```csv_file_2.csv```

### Запуск
Тестировалось на Python 3.11.1.
Внешних зависимостей нет.
```
python run.py
```

### Структура

Основне модули:
```
.
├── ngxmlzip
│   ├── create.py       функции для создания zip файлов с xml
│   ├── process.py      функции для обработки и формирования результата
└── run.py
```

Настройка:
В файле run.py возможно указать следующие константы
```python
MAX_OBJECTS_IN_XML = 10 # максимальное количество объектов в XML файле
XML_FILES_IN_ZIP = 100  # количество xml файлов в одном zip файле   
ZIP_FILES = 50  # количество xml файлов в одном zip файле   
ZIP_DIRECTORY = "zip-files"  # директория с zip файлами 
CSV_FILE_1 = "csv_file_1.csv" # csv файл результата №1
CSV_FILE_2 = "csv_file_2.csv" # csv файл результата №2
```
### [todo] Удобство использования
Для повышения удобства можно реализовать проброску параметров через командную строку. На уровне дизайна модулей этот функционал уже заложен. 

## [todo] Оптимизация 
Цель: разделить по процессам работу с файловой системой и обработку данных. Таким образом получиться распараллелить потоки для чтения/записи данных их обработки для уменьшения степени блокировки.

Целевой дизайн
- 3 очереди: названия zip файлов, xml данные, очередь результатов
- 4 воркера:
  - открытие zip файла
  - чтение xml файла
  - обработка результата
  - дозапись результата в файлы результатов 

Сценарий:
- Основной поток формирует список zip файлов и складывает их имена в очередь
- Дочерний процессы (-с) берут данные из очереди названий zip файлов и складывают содержание xml файлов в очередь xml данных. Один zip файл на отдельный процесc (чтение zip файла дорогое, но можно и попробовать тоже распареллить для случая если внутри зипа ожидается еще больше xml файлов). 
- Дочерние процессы забирают данные из очереди с содержанием xml файлов, обрабатывают и записывают результат в очередь результатов
- Отдельный процесс (-ы) забирает данные из очереди результатов чанками и дозаписывают csv файлы

По идее, в реализованных модулях все функции и структуры данных уже готовы для поддержки такого сценария. 

Реализация:
- multiprocessing.Pool
- multiprocessing.Queue

План по реализации: 

Итерация 1 (очередь с данными xml): 
  - В основном потоке открывать все zip файлы один за одним и возвращать данные в очередь
  - Дочерние процессы забирают данные из очереди с содержанием xml файлов, обрабатывают и записывают результат

Итерация 2 (2 очереди: названия zip файлов, xml данные): 
  - Основной поток формирует список zip файлов и складывает их имена в очередь
  - Дочерний процесс (-ы) берет данные из очереди названий zip файлов и складывает содержание в очередь xml данных
  - Дочерние процессы забирают данные из очереди с содержанием xml файлов и обрабатывают обрабатывают и записывают результат

Итерация 3: реализация целевого сценария


Таким образом получиться разделить по процессам работу с файловой системой и обработку данных.


